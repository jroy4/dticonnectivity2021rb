{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c950e44-bb5e-4975-9e68-e069fdc3a188",
   "metadata": {},
   "source": [
    "This code is in-progress. Its purpose is to read in the tractography data into a dataframe and convert it into a format compatible for machine learning (wide format) -- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3962c4f-4bae-49cc-9f40-debfe535b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rd\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import theano.tensor as T\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "from matplotlib import pyplot as plt\n",
    "from nipy import load_image, save_image\n",
    "from nipy.core.api import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import plot_roc_curve, auc \n",
    "from sklearn.base import clone\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1072c-1d95-46b2-a5e6-95e3688e56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1d8c6-5504-410c-bafd-c174cb454e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't use this anymore, but keeping incase I want it later\n",
    "def convert_ca(date_string):\n",
    "    #input format: MM/DD/YY\n",
    "    month = int(date_string.split('/')[0])\n",
    "    day = int(date_string.split('/')[1])\n",
    "    year = int(date_string.split('/')[2])\n",
    "    #return format: YYYYMMDD\n",
    "    return (year+2000)*10000 + month*100 + day\n",
    "    \n",
    "\n",
    "def load_tract_data(tractdata):\n",
    "    filename = tractdata.split('/')[-1]\n",
    "    \n",
    "    StudyID = int(filename.split('_')[0])\n",
    "    MR_num = filename.split('_')[1]\n",
    "    Scan_num = filename.split('_')[2]\n",
    "    if Scan_num[-4:]=='.txt':\n",
    "        Scan_num = Scan_num[:-4]\n",
    "    data = pd.read_csv(tractdata, sep='\\t')\n",
    "#     data = data.drop(['Unnamed: 21'], axis=1) #old way of doing the below\n",
    "    data = data.iloc[: , :-1] #get rid of empty last column\n",
    "    data = data.rename(columns={'Tract Name': 'field'})\n",
    "    data['field'] = data['field'].str.replace(' ', '_')\n",
    "    data = pd.melt(data, id_vars=['field'],\n",
    "            var_name='tract', value_name='value')\n",
    "    data['tract.field'] = data['tract'] + '.' + data['field']\n",
    "    data['StudyID'] = StudyID\n",
    "    data['MR_num'] = MR_num\n",
    "    data['Scan_num'] = Scan_num\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78d2f1-20da-4f96-8c53-08f6151cb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eddy_corrected = '/data/Reback-DTI/resources/stats/Eddy_corrected/'\n",
    "Noncorrected = '/data/Reback-DTI/resources/stats/Noncorrected'\n",
    "Batch1 = os.path.join(Eddy_corrected, 'Batch1')\n",
    "Batch2 = os.path.join(Eddy_corrected, 'Batch2')\n",
    "Batch3 = os.path.join(Eddy_corrected, 'Batch3')\n",
    "\n",
    "Batch_dir  = Batch3\n",
    "input_Dir  = os.path.join(Batch_dir, 'Input_dir')\n",
    "output_dir = os.path.join(Batch_dir, 'Output_dir')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783b1d6-9eb1-462e-863d-174a2e81b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_files = [os.path.join(input_Dir, X) for X in os.listdir(input_Dir) if X[-4:]=='.txt']\n",
    "patient_datas = []\n",
    "for file in stat_files:\n",
    "    data = load_tract_data(file)\n",
    "    patient_datas.append(data)\n",
    "diffusion_data = pd.concat(patient_datas)\n",
    "wide_data = diffusion_data.pivot(index=['StudyID', 'MR_num', 'Scan_num'], columns=['tract.field'], values='value')\n",
    "wide_data = wide_data.reset_index()\n",
    "# wide_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27367a-bc11-482d-bf13-4523636e65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide_data.to_csv('/data/Reback-DTI/wide_data.csv', index=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f0d57-0d6e-46c9-9cba-69699b8eea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_data(src):\n",
    "    #note, i had to \"clean\" some of the data manually before using it in this program\n",
    "    # I only converted y=1 and n=0 and ? = ' '\n",
    "    clinical_data_file = src\n",
    "    clinical_data = pd.read_csv(clinical_data_file, sep=',')\n",
    "    clinical_data = clinical_data.dropna(how='all')\n",
    "    clinical_data['Study ID'] = clinical_data['Study ID'].astype(int)\n",
    "    #necessary to merge with data elsewhere\n",
    "    clinical_data = clinical_data.rename(columns={\"Study ID\": \"StudyID\"})\n",
    "    clinical_data = clinical_data.drop(['Unnamed: 20'], axis=1)\n",
    "    clinical_data.columns = clinical_data.columns.str.replace(' ', '_')\n",
    "    #convert date to an number\n",
    "    #clinical_data['Date CA'] = clinical_data['Date CA'].map(lambda x: convert_ca(x))\n",
    "    clinical_data['Date_CA'] = pd.to_datetime(clinical_data['Date_CA'], dayfirst=False)\n",
    "    clinical_data['Date_CA'] = clinical_data['Date_CA'].astype(int) / 10**9\n",
    "    #REPLACE EMPTY CELLS WITH NaN\n",
    "    clinical_data = clinical_data.replace(' ', np.NaN)\n",
    "    for var in clinical_data.columns:\n",
    "        try:\n",
    "            clinical_data[var] = clinical_data[var].astype('float64')\n",
    "        except Exception as e:\n",
    "            print(f'{str(e)} for column: {var}')\n",
    "    clinical_data = clinical_data.fillna(clinical_data.mean())\n",
    "    return clinical_data\n",
    "\n",
    "clinical_data_src = '/data/Reback-DTI/resources/clinicaldata/clinicaldata.csv'\n",
    "clinical_data = load_clinical_data(clinical_data_src)\n",
    "clinical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d8dd2-d260-42c8-87a9-8cd6fbb09efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_stat_data = wide_data.merge(clinical_data, on ='StudyID', how = 'inner')\n",
    "#Note: the following are subjects without associated clinical data\n",
    "#notIncl = wide_data[~wide_data.StudyID.isin(clin_stat_data.StudyID)]\n",
    "clin_stat_data.to_csv(os.path.join(output_dir, 'clin_stat_data.csv'), index=False, sep=',')\n",
    "clin_stat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f89d0-5454-451c-8eba-553cb7fc0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the closest post-CA scan\n",
    "def load_MR_Date_data(MR_Date_src):\n",
    "    MR_Date_data = pd.read_csv(MR_Date_src, sep=',')\n",
    "    MR_Date_data = MR_Date_data.rename(columns={\"MR ID\": \"MR_ID\"})\n",
    "    MR_Date_data = MR_Date_data.rename(columns={\"Date\": \"ScanDate\"})\n",
    "    #convert date to an number\n",
    "    MR_Date_data['ScanDate'] = pd.to_datetime(MR_Date_data['ScanDate'], dayfirst=False)\n",
    "    MR_Date_data['ScanDate'] = MR_Date_data['ScanDate'].astype(int)/ 10**9\n",
    "    #MR_Date_data['ScanDate'] = MR_Date_data['ScanDate'].map(lambda x: convert_ca(x)) \n",
    "    return MR_Date_data\n",
    "\n",
    "def filterDuplicates(in_data):\n",
    "    #get MR dates\n",
    "    MR_Date_src = '/data/Reback-DTI/resources/clinicaldata/Reback-MRIs.csv'   \n",
    "    MR_Date_data = load_MR_Date_data(MR_Date_src)\n",
    "    \n",
    "    #add MR dates to running data\n",
    "    out_data = in_data\n",
    "    out_data['MR_ID'] = out_data['StudyID'].map(str).str.zfill(4) + '_' + out_data['MR_num'].map(str)\n",
    "    out_data = out_data.merge(MR_Date_data, on ='MR_ID', how = 'inner')\n",
    "    \n",
    "    #remove scans that happens before the CA\n",
    "    out_data['TCA2MR'] = out_data['ScanDate'] - out_data['Date_CA']\n",
    "    out_data1 = out_data[out_data['TCA2MR'] < 0]\n",
    "    out_data = out_data[out_data['TCA2MR'] >= 0]\n",
    "    #The following are MRIs taken before CA\n",
    "    #print(out_data1)\n",
    "    \n",
    "    \n",
    "    #sort by time and only keep the scans closest to the CA\n",
    "    out_data = out_data.sort_values(by=['StudyID','TCA2MR'], ascending=True)\n",
    "    #The following are duplicate scans\n",
    "    #duplicate = out_data[out_data.duplicated('StudyID')]\n",
    "    #print(duplicate)\n",
    "    out_data = out_data.drop_duplicates(subset = ['StudyID'], keep='first')\n",
    "\n",
    "    \n",
    "    return out_data\n",
    "\n",
    "RF_data = filterDuplicates(clin_stat_data)\n",
    "RF_data.to_csv(os.path.join(output_dir, 'RF_data.csv'), index=False, sep=',')\n",
    "RF_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad55f8-3811-41c0-b770-6971d9a465a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75fd4c-c6d1-4325-b76e-07a618b0649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the group to the whole frame\n",
    "#temporary to run ANOVA\n",
    "RF_data.loc[(RF_data['Age_y'] >= 0) & (RF_data['Age_y'] < 1), 'AgeGroup'] = '0' \n",
    "RF_data.loc[(RF_data['Age_y'] >= 1) & (RF_data['Age_y'] < 5), 'AgeGroup'] = '1' \n",
    "RF_data.loc[(RF_data['Age_y'] >= 5) , 'AgeGroup'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18288a72-e01e-4180-985c-0c1b130e3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_data.to_csv(os.path.join(output_dir, 'RF_ageStrat.csv'), index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee52733-02be-43a5-a8ef-ac0a3f396a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAge0_1 = RF_data[(RF_data['Age_y'] >= 0) & (RF_data['Age_y'] < 1)]\n",
    "dataAge1_5 = RF_data[(RF_data['Age_y'] >= 1) & (RF_data['Age_y'] < 5)]\n",
    "dataAge5_  = RF_data[(RF_data['Age_y'] >= 5)]\n",
    "dataAge1_  = RF_data[(RF_data['Age_y'] >= 1)]\n",
    "dataAge0_5 = RF_data[(RF_data['Age_y'] >= 0) & (RF_data['Age_y'] < 5)]\n",
    "dataAge0_2 = RF_data[(RF_data['Age_y'] >= 0) & (RF_data['Age_y'] < 2)]\n",
    "\n",
    "base_dir = output_dir\n",
    "strat_dir = os.path.join(base_dir, 'strats')\n",
    "\n",
    "all_ages_dir = os.path.join(strat_dir, 'all_ages/')\n",
    "if not os.path.exists(all_ages_dir):\n",
    "    os.makedirs(all_ages_dir)\n",
    "age0_1_dir   = os.path.join(strat_dir, 'ages0_1/')\n",
    "if not os.path.exists(age0_1_dir):\n",
    "    os.makedirs(age0_1_dir)\n",
    "age1_5_dir   = os.path.join(strat_dir, 'ages1_5/')\n",
    "if not os.path.exists(age1_5_dir):\n",
    "    os.makedirs(age1_5_dir)\n",
    "age5__dir    = os.path.join(strat_dir, 'ages5_/')\n",
    "if not os.path.exists(age5__dir):\n",
    "    os.makedirs(age5__dir)\n",
    "age1__dir    = os.path.join(strat_dir, 'ages1_/')\n",
    "if not os.path.exists(age1__dir):\n",
    "    os.makedirs(age1__dir)\n",
    "age0_5_dir   = os.path.join(strat_dir, 'ages0_5/')\n",
    "if not os.path.exists(age0_5_dir):\n",
    "    os.makedirs(age0_5_dir)\n",
    "age0_2_dir   = os.path.join(strat_dir, 'ages0_2/')\n",
    "if not os.path.exists(age0_2_dir):\n",
    "    os.makedirs(age0_2_dir)\n",
    "\n",
    "RF_data.to_csv(os.path.join(all_ages_dir,   'all_ages.csv'),   index=True, sep=',')\n",
    "dataAge0_1.to_csv(os.path.join(age0_1_dir,  'ages0_1.csv'),    index=True, sep=',')\n",
    "dataAge1_5.to_csv(os.path.join(age1_5_dir,  'ages1_5.csv'),    index=True, sep=',')\n",
    "dataAge5_.to_csv(os.path.join(age5__dir,    'ages5_.csv'),     index=True, sep=',')\n",
    "dataAge1_.to_csv(os.path.join(age1__dir,    'ages1_.csv'),     index=True, sep=',')\n",
    "dataAge0_5.to_csv(os.path.join(age0_5_dir,  'ages0_5.csv'),    index=True, sep=',')\n",
    "dataAge0_2.to_csv(os.path.join(age0_2_dir,  'ages0_2.csv'),    index=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f818e-56ca-4d4e-a214-38ec3700ed05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e0abdf3-12d9-4751-8568-f4459778bffd",
   "metadata": {},
   "source": [
    "********************************\n",
    "**Now begins the code necessary to make/run the RF\n",
    "********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268dec5-5d43-4a9c-bf23-dfde12ee7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_predictions, test_labels, test_probs,\n",
    "                   train_predictions, train_labels, train_probs, model_name):\n",
    "    baseline = {}\n",
    "    baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    test_results['recall'] = recall_score(test_labels, test_predictions)\n",
    "    test_results['precision'] = precision_score(test_labels, test_predictions)\n",
    "    test_results['roc'] = roc_auc_score(test_labels, test_probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    \n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(test_results[metric], 2)}, Train: {round(train_results[metric], 2)}')\n",
    "        \n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, test_probs)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    #Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label='baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label='model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate');\n",
    "    plt.ylabel('True Positive Rate');\n",
    "    plt.title('ROC Curve')  \n",
    "    plt.savefig(f'/data/Reback-DTI/resources/stats/MODEL-{model_name}_AUROC')\n",
    "    plt.close()\n",
    "    \n",
    "def find_best_model_and_train_on_split(predictors, outcome, test_size=0.2, iterations=1000):\n",
    "\n",
    "\n",
    "    train, test, train_labels, test_labels = train_test_split(predictors, outcome, \n",
    "                                                          stratify = outcome, \n",
    "                                                         test_size = test_size)\n",
    "\n",
    "    print(f\"Train size: {train.shape}\")\n",
    "    print(f\"Test size: {test.shape}\")\n",
    "\n",
    "\n",
    "# Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': np.linspace(50, 1000, num=100).astype(int),\n",
    "        'max_depth': [None] + list(np.linspace(5, 30, num=30).astype(int)),\n",
    "        'max_features': ['auto', 'sqrt', None] + list(np.arange(0.3, 1, 0.1)),\n",
    "        'max_leaf_nodes': [None] + list(np.linspace(5, 50, 50).astype(int)),\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'bootstrap': [True]\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Estimator for use in random search\n",
    "    estimator = RandomForestClassifier()\n",
    "\n",
    "    # Create the random search model\n",
    "    rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                            scoring = 'roc_auc', cv = 2, \n",
    "                            n_iter = iterations, verbose = 1)\n",
    "\n",
    "    # Fit \n",
    "    rs.fit(train, train_labels)\n",
    "    best_params = rs.best_params_\n",
    "    \n",
    "    print(f'Best parameters: {best_params}')\n",
    "\n",
    "    best_model = rs.best_estimator_\n",
    "\n",
    "    n_nodes = []\n",
    "    max_depths = []\n",
    "\n",
    "    for ind_tree in best_model.estimators_:\n",
    "        n_nodes.append(ind_tree.tree_.node_count)\n",
    "        max_depths.append(ind_tree.tree_.max_depth)\n",
    "\n",
    "    print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "    print(f'Average maximum depth {int(np.mean(max_depths))}')\n",
    "\n",
    "    train_rf_predictions = best_model.predict(train)\n",
    "    train_rf_probs = best_model.predict_proba(train)[:,1]\n",
    "\n",
    "    test_rf_predictions = best_model.predict(test)\n",
    "    test_rf_probs = best_model.predict_proba(test)[:,1]\n",
    "    \n",
    "    print(train_rf_predictions)\n",
    "    print(train_labels)\n",
    "    print(train_rf_probs)\n",
    "    print(test_rf_predictions)\n",
    "    print(test_labels)\n",
    "    print(test_rf_probs)\n",
    "    evaluate_model(test_rf_predictions, test_labels, test_rf_probs,\n",
    "                   train_rf_predictions, train_labels, train_rf_probs, 'TEST_RF')\n",
    "    \n",
    "    fi_model = pd.DataFrame({'feature': list(predictors.columns),\n",
    "                             'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return best_params, best_model, fi_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f57a3-97af-4e07-968f-1be2aed90705",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = ['mortality_6mos', 'pcpcgp_6mo']\n",
    "outcome_data = RF_data[outcomes]\n",
    "predictors_data = RF_data.drop(outcomes, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c71d1d-a096-42bf-9867-5be95cf9942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params, best_model, fi_model = find_best_model_and_train_on_split(predictors_data, outcome_data[outcomes[1]])\n",
    "#fi_model.to_csv('/data/Reback-DTI/feature_imprtances.csv', index=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba3090-d5d5-4c71-84ab-7e5d57be0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime_object = datetime.datetime.now()\n",
    "print(datetime_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e3c2f-19ca-4118-af63-3e3dd8dd3bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18714665-cfd4-4458-bc9b-62206c38eed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
